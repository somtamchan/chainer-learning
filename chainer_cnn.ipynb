{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChainerでCNNしたった\n",
    "https://qiita.com/yoyoyo_/items/14f243c058f5a1e7044b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "from chainer.dataset import convert\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import serializers\n",
    "from chainer.links.caffe import CaffeFunction\n",
    "\n",
    "import glob, random, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## fine-tuning setting\n",
    "FT_MODEL = 'result/model.npz'\n",
    "\n",
    "## paths to train image directory and test image one\n",
    "HOME = os.path.expanduser('~')\n",
    "TRAIN_PATH = HOME + '/Deeplearning/Images2/Train/'\n",
    "TEST_PATH = HOME + '/Deeplearning/Images2/Test/'\n",
    "\n",
    "## model name for save trained, and model name for load testing\n",
    "SAVE_MODEL = 'result/model2'\n",
    "LOAD_MODEL = 'result/model2.npz'\n",
    "\n",
    "## Train hyper-parameters\n",
    "CLASS = 2\n",
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "MINIBATCH_SIZE = 20\n",
    "LEARN_RATE = 0.001\n",
    "EPOCH = 50\n",
    "GPU = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mynet(chainer.Chain):\n",
    "\n",
    "    def __init__(self, class_labels=10):\n",
    "        super(Mynet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1_1 = L.Convolution2D(None, 16, ksize=5, pad=2, nobias=True)\n",
    "            self.conv1_2 = L.Convolution2D(None, 16, ksize=5, pad=2, nobias=True)\n",
    "            self.conv2_1 = L.Convolution2D(None, 32, ksize=3, pad=1, nobias=True)\n",
    "            self.conv2_2 = L.Convolution2D(None, 32, ksize=3, pad=1, nobias=True)\n",
    "            self.fc1 = L.Linear(None, 512, nobias=True)\n",
    "            self.fc2 = L.Linear(None, class_labels, nobias=True)      \n",
    "\n",
    "    def __call__(self, x):\n",
    "        conv1_1 = self.conv1_1(x)\n",
    "        conv1_1 = F.relu(conv1_1)\n",
    "        conv1_2 = self.conv1_2(conv1_1)\n",
    "        conv1_2 = F.relu(conv1_2)\n",
    "        pool1 = F.max_pooling_2d(conv1_2, ksize=2, stride=2)\n",
    "        conv2_1 = self.conv2_1(pool1)\n",
    "        conv2_1 = F.relu(conv2_1)\n",
    "        conv2_2 = self.conv2_2(conv2_1)\n",
    "        conv2_2 = F.relu(conv2_2)\n",
    "        pool2 = F.max_pooling_2d(conv2_2, ksize=2, stride=2)\n",
    "        fc1 = self.fc1(pool2)\n",
    "        fc1 = F.relu(fc1)\n",
    "        fc2 = self.fc2(fc1)\n",
    "        return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "\n",
    "def load_images(dataset_path, shuffle=True):\n",
    "    filepaths_jpg = glob.glob(dataset_path + '/*.jp*g')\n",
    "    filepaths_png = glob.glob(dataset_path + '/*.png')\n",
    "    filepaths = filepaths_jpg + filepaths_png\n",
    "    filepaths.sort()\n",
    "    datasets = []\n",
    "    for filepath in filepaths:\n",
    "        img = Image.open(filepath).convert('RGB') ## Gray->L, RGB->RGB\n",
    "        img = img.resize((INPUT_WIDTH, INPUT_HEIGHT))\n",
    "\n",
    "        x = np.array(img, dtype=np.float32)\n",
    "        ## Normalize [0, 255] -> [0, 1]\n",
    "        x = x / 255.\n",
    "        ## Reshape image to input shape of CNN\n",
    "        x = x.transpose(2, 0, 1)\n",
    "        #x = x.reshape(3, INPUT_HEIGHT, INPUT_WIDTH)\n",
    "\n",
    "        ## Get label(ground-truth) from file name path \n",
    "        label = int(filepath.split('/')[-1].split('_')[0])\n",
    "        t = np.array(label, dtype=np.int32)\n",
    "        datasets.append((x,t))\n",
    "\n",
    "    if shuffle: random.shuffle(datasets)\n",
    "\n",
    "    return datasets, filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_train(train_Model, ft_Model=None):\n",
    "\n",
    "    print('\\nmodel training start!!\\n')\n",
    "    print('# GPU: {}'.format(GPU))\n",
    "    print('# Minibatch-size: {}'.format(MINIBATCH_SIZE))\n",
    "    print('# Epoch: {}'.format(EPOCH))\n",
    "    print('# Learnrate: {}'.format(LEARN_RATE))\n",
    "\n",
    "    ## Load train and test images \n",
    "    train, _ = load_images(TRAIN_PATH)\n",
    "    test, _ = load_images(TEST_PATH)\n",
    "\n",
    "    if len(train) < 1 or len(test) < 1:\n",
    "        raise Exception('train num : {}, test num: {}'.format(len(train), len(test)))\n",
    "\n",
    "    train_count = len(train)\n",
    "    test_count = len(test)\n",
    "\n",
    "    print('# Train images: {}'.format(train_count))\n",
    "    print('# Test images: {}\\n'.format(test_count))\n",
    "\n",
    "    ## model define\n",
    "    model = train_Model\n",
    "\n",
    "    ## Set GPU device\n",
    "    if GPU >= 0:\n",
    "        chainer.cuda.get_device(GPU).use()\n",
    "        model.to_gpu()\n",
    "\n",
    "    ## Fine-tuning from pre-trained model\n",
    "    if ft_Model is not None:\n",
    "        #orig = pickle.load(open(FT_MODEL, \"rb\"))\n",
    "\n",
    "        ## train model parameter initialization\n",
    "        dummy = np.zeros((3, INPUT_HEIGHT, INPUT_WIDTH))\n",
    "        dummy = dummy[None, ...]\n",
    "        dummy = np.array(dummy, dtype=np.float32)\n",
    "        dummy = chainer.Variable(dummy)\n",
    "        _, _ = model(dummy)\n",
    "\n",
    "        ## load fine-tuning original model and copy it to train model\n",
    "        print('\\nFine-tuning from {}\\n'.format(FT_MODEL))\n",
    "        orig = ft_Model\n",
    "        serializers.load_npz(FT_MODEL, orig)\n",
    "        fine_tuning(orig, model)\n",
    "\n",
    "        #serializers.load_npz(FT_MODEL, model)\n",
    "\n",
    "\n",
    "    ## Set Optimizer\n",
    "    optimizer = chainer.optimizers.MomentumSGD(LEARN_RATE)\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(train, MINIBATCH_SIZE)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, MINIBATCH_SIZE, repeat=False, shuffle=False)\n",
    "\n",
    "    ## Training start!!\n",
    "    start = time.time()\n",
    "\n",
    "    print('epoch  train_loss  train_accuracy  test_loss  test_accuracy  Elapsed-Time')\n",
    "\n",
    "    while train_iter.epoch < EPOCH:\n",
    "\n",
    "        batch = train_iter.next()\n",
    "        # Reduce learning rate by 0.5 every 25 epochs.\n",
    "        #if train_iter.epoch % 25 == 0 and train_iter.is_new_epoch:\n",
    "        #    optimizer.lr *= 0.5\n",
    "        #    print('Reducing learning rate to: ', optimizer.lr)\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "\n",
    "        x_array, t_array = convert.concat_examples(batch, GPU)\n",
    "        x = chainer.Variable(x_array)\n",
    "        t = chainer.Variable(t_array)\n",
    "\n",
    "        y, _ = model(x)\n",
    "        loss_train = F.softmax_cross_entropy(y, t)\n",
    "        accuracy_train = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss_train.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        train_losses.append(chainer.cuda.to_cpu(loss_train.data))\n",
    "        accuracy_train.to_cpu()\n",
    "        train_accuracies.append(accuracy_train.data)\n",
    "\n",
    "        if train_iter.is_new_epoch:\n",
    "            #print('epoch: ', train_iter.epoch)\n",
    "            #print('train mean loss: {:.2f}, accuracy: {:.2f}'.format( sum_loss_train / train_count, sum_accuracy_train / train_count))\n",
    "            # evaluation\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            sum_accuracy_test = 0\n",
    "            sum_loss_test = 0\n",
    "            #model.predictor.train = False\n",
    "            for batch in test_iter:\n",
    "                x_array, t_array = convert.concat_examples(batch, GPU)\n",
    "                x = chainer.Variable(x_array)\n",
    "                t = chainer.Variable(t_array)\n",
    "\n",
    "                y = model(x)\n",
    "\n",
    "                loss_test = F.softmax_cross_entropy(y, t)\n",
    "                accuracy_test = F.accuracy(y, t)\n",
    "\n",
    "                test_losses.append(chainer.cuda.to_cpu(loss_test.data))\n",
    "                accuracy_test.to_cpu()\n",
    "                test_accuracies.append(accuracy_test.data)\n",
    "\n",
    "\n",
    "            test_iter.reset()\n",
    "            #model.predictor.train = True\n",
    "\n",
    "            print('{:>5}  {:^10.4f}  {:^14.4f}  {:^9.4f}  {:^13.4f}  {:^12.2f}'.format(train_iter.epoch, np.mean(train_losses), np.mean(train_accuracies), np.mean(test_losses), np.mean(test_accuracies), time.time()-start))\n",
    "\n",
    "\n",
    "    print('\\ntraining finished!!\\n')\n",
    "\n",
    "    ## Save the model and the optimizer\n",
    "    print('save model start!!\\n')\n",
    "    directory = SAVE_MODEL.split('/')[0]\n",
    "    if not os.path.exists(directory):\n",
    "        os.system('mkdir {}'.format(directory))\n",
    "        print('make outout model directory --> {}'.format(directory))\n",
    "\n",
    "    serializers.save_npz(SAVE_MODEL + '.npz', model)\n",
    "    print('save the model --> {}'.format(SAVE_MODEL + '.npz') )\n",
    "    serializers.save_npz(SAVE_MODEL + '.state', optimizer)\n",
    "    print('save the optimizer --> {}'.format(SAVE_MODEL + '.state'))\n",
    "    pickle.dump(model, open(SAVE_MODEL + '.pkl', 'wb'))\n",
    "    print('save the model --> {}'.format(SAVE_MODEL + '.pkl'))\n",
    "\n",
    "    print('\\nmodel save finished!!\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
